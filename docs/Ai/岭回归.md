# 岭回归

岭回归（Ridge Regression）是一种处理多重共线性问题的线性模型正则化方法，与套索回归类似，但采用的是L2正则化，即模型系数平方和的惩罚项。岭回归通过在损失函数中加入正则项，来限制模型系数的大小，从而避免过拟合，提高模型的稳定性。与套索回归相比，岭回归倾向于收缩所有系数而不是将其直接推向0，因此它通常不会进行特征选择。

### 应用场景示例

假设你正在分析一个教育数据集，试图预测学生的学术表现（如考试分数），而使用的特征包括学生的家庭作业完成率、课堂参与度、课外活动参与情况等多个高度相关的指标。在这种情况下，岭回归可以有效应对这些特征间的多重共线性问题，提供一个更稳定且泛化能力更强的预测模型，而不会因为某些特征间的高度相关导致模型系数估计不稳定。

### 训练模型的步骤

训练岭回归模型的过程与套索回归相似，主要包括数据预处理、模型训练、模型评估等步骤。下面是使用Python的Scikit-learn库进行岭回归的示例代码：

::: code-group
```python
from sklearn.datasets import load_boston
from sklearn.linear_model import Ridge
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, r2_score

# 加载波士顿房价数据集（这里用作示例，实际应选择与教育成绩预测相关的数据）
boston = load_boston()
X, y = boston.data, boston.target

# 数据预处理：标准化
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# 划分训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

# 网格搜索寻找最佳的alpha参数
ridge = Ridge()
alphas = np.logspace(-4, 4, 100)  # 定义alpha的候选值范围
param_grid = {'alpha': alphas}
grid_search = GridSearchCV(ridge, param_grid, cv=5, scoring='neg_mean_squared_error')
grid_search.fit(X_train, y_train)

# 获取最佳alpha值
best_alpha = grid_search.best_params_['alpha']
print(f"Best alpha: {best_alpha}")

# 使用最佳alpha重新训练模型
ridge_best = Ridge(alpha=best_alpha)
ridge_best.fit(X_train, y_train)

# 预测并评估
y_pred = ridge_best.predict(X_test)
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
print(f"Mean Squared Error: {mse}")
print(f"R^2 Score: {r2}")

# 查看系数
coef = ridge_best.coef_
print("Coefficients:", coef)
```
:::
