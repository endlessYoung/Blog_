# 梯度下降

## 1. 什么是梯度下降

梯度下降是一种优化算法，它通过迭代的方式逐步调整模型参数，以最小化损失函数活优化目标函数。


::: tip
注意：
**在本文中的目标函数指的是损失函数(Loss function)。在监督学习中，常见的损失函数包括均方误差(Mean Squared Error, MSE)、交叉熵误差(Cross Entropy Loss)、对数损失(Log Loss)。这些损失函数描述了模型预测值与真实标签之间的差异。**
:::

梯度下降的步骤：
1. **初始化参数**：首先，需要给定模型的初始参数值。
2. **计算梯度**：在当前参数下，计算目标函数相对于参数的梯度（也就是导数）。梯度表示了目标函数在当前参数值处的变化率和方向。
3. **更新参数**：沿着梯度的反方向调整参数值，以降低目标函数的值。这个调整的大小由学习率（Learning rate）控制，学习率决定了每一步更新参数的幅度。
4. **重复迭代**：重复步骤2和步骤3，直到达到停止条件，比如达到最大迭代次数、目标函数收敛或者梯度接近于0。

梯度下降的核心思想是通过不断地沿着目标函数梯度的反方向调整参数，逐步接近或达到最优解。他是一种迭代的优化算法，在每一步都朝着减小目标函数值的方向前进。

梯度下降算法有多种变体，包括`批量梯度下降(Batch Gradient Descent)`、`随机梯度下降(Stochastic Gradient Descent)`和`小批量梯度下降(Mini-batch Gradient Descent)`等，它们在参数更新的方式和效率上略有不同，但是基本思想相似。