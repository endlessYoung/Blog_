# 逻辑回归

## 1.什么是逻辑回归
逻辑回归（Logistic Regression）是一种用于解决分类问题的统计学习方法。尽管其名称中包含“回归”一词，但逻辑回归实际上是一种分类算法，常用于解决二分类问题，也可以扩展到多分类问题。

## 2.原理

逻辑回归的基本思想是将输入特征与相应的输出标签之间的关系建模为概率分布。具体来说，逻辑回归使用 logistic 函数（也称为 sigmoid 函数）将输入特征的线性组合映射到[0, 1]区间上，表示样本属于某一类的概率。数学上，sigmoid 函数的形式如下：

$\sigma(z) = \frac{1}{1 + e^{-z}}$

其中 $( z )$ 是输入特征的线性组合，即 $( z = \theta_0 + \theta_1x_1 + \theta_2x_2 + \ldots + \theta_nx_n )$，其中 $( \theta )$ 是模型的参数（权重），$( x )$ 是输入特征。

逻辑回归的模型可以表示为：

$h_{\theta}(x) = \sigma(\theta^T x)$

其中 $( h_{\theta}(x) )$ 表示预测样本属于正类的概率，$( \theta^T )$ 表示参数向量的转置，$( x )$ 是输入特征向量。

在训练逻辑回归模型时，通常使用最大似然估计或梯度下降等优化算法来最大化训练数据的似然函数，从而求解最优的参数 $( \theta )$。训练完成后，就可以使用学习到的参数来进行预测，并根据预测概率进行分类。

## 3.逻辑回归的损失函数

逻辑回归通常采用对数似然损失函数（或交叉熵损失函数）作为优化目标，其表达式为：

  $L = -\sum_{i=1}^{m}[y_i\log(p_i) + (1-y_i)\log(1-p_i)]$
  其中，$(m)$ 是样本数量，$(y_i)$ 是实际标签（0或1），$(p_i)$ 是模型预测的该样本属于正类的概率。

- **优化算法**：为了找到最优的模型参数，通常使用梯度下降法或其变种（如`随机梯度下降SGD`、`批量梯度下降BGD`、小`批量梯度下降MBGD`等）来最小化损失函数。

### 应用场景

逻辑回归因其简单高效，在很多领域都有应用，包括但不限于：
- 金融领域：信用评分、欺诈检测。
- 医疗领域：疾病诊断。
- 营销：客户响应预测、用户留存预测。
- 社会科学：选举结果预测、社会调查数据分析。

### 代码示例

::: code-group
``` python
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

# 加载鸢尾花数据集
iris = load_iris()
X = iris.data
y = iris.target

# 将数据集划分为训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 创建并训练逻辑回归模型
logistic_reg = LogisticRegression()
logistic_reg.fit(X_train, y_train)

# 在测试集上进行预测
y_pred = logistic_reg.predict(X_test)

# 计算准确率
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)

```
:::

在这个示例中，首先加载了鸢尾花数据集，并将其划分为训练集和测试集。然后，创建了一个逻辑回归模型，并使用训练集对其进行训练。接下来，使用训练好的模型在测试集上进行预测，并计算了预测结果的准确率。

### 总结

逻辑回归是一种基础且强大的分类算法，尽管名字中有“回归”二字，但它实际上是一种分类方法。它通过非线性转换（Sigmoid函数）将线性模型的输出转化为概率预测，适用于处理二分类问题，并且易于理解和实现。
