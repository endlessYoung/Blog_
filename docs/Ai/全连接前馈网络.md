# 全连接前馈网络(FNN)

**全连接前馈网络**（Fully Connected Feedforward Network）是一种基本的神经网络架构，也被称为**多层感知机**（Multilayer Perceptron，MLP）。它包含一个或多个隐藏层，每个隐藏层都由多个神经元组成，每个神经元与上一层的所有神经元相连接，因此称为“全连接”。在全连接神经网络中，对n-1层和n层而言，n-1层的任意一个节点，都和第n层所有节点有连接。即第n层的每个节点在进行计算的时候，激活函数的输入是n-1层所有节点的加权，这个激活函数是非线性的。它的缺点就是权重太多了，计算量很大。
它可作用于大多数场景。

全连接前馈网络的基本结构和运作原理：

1. **输入层（Input Layer）**： 接受输入数据的层。每个输入特征对应于输入层中的一个神经元。输入层的神经元数量取决于输入数据的维度。
2. **隐藏层（Hidden Layers）**： 每个隐藏层包含多个神经元，每个神经元都与上一层（可能是输入层或前一个隐藏层）的所有神经元连接。隐藏层的数量和每个隐藏层中的神经元数量是根据任务和模型设计而定的超参数。
3. **输出层（Output Layer）**： 生成模型的输出。输出层的神经元数量通常取决于任务的类型，例如分类问题的输出层通常具有与类别数量相同的神经元，回归问题的输出层通常只有一个神经元。
4. **激活函数（Activation Functions）**： 在隐藏层和输出层中，每个神经元通常都会应用一个非线性的激活函数，如ReLU（Rectified Linear Unit）、Sigmoid、tanh等。激活函数帮助网络引入非线性，使其能够学习复杂的函数映射关系。
5. **权重和偏置（Weights and Biases）**： 神经网络中的连接（或称为边）都有一个相关联的权重，它表示了两个神经元之间连接的强度。每个神经元还有一个偏置项，用于调整神经元的激活阈值。这些权重和偏置是模型在训练过程中学习到的参数。

**总结**：全连接前馈网络通过前向传播算法来计算输入数据通过网络后得到的输出。在训练阶段，通过反向传播算法来调整网络参数以最小化损失函数，从而使网络能够对输入数据进行准确的预测。